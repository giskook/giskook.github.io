---
layout: post
title: "【译】kafka简介"
date: 2021-11-09 20:02:24+08:00
categories: work tech
---

----------------

[原文地址](https://kafka.apache.org/documentation/#design) 关注design部分

----------------

### 动机

设计kafka是为了处理所有的很多大公司可能会有的实时数据流。为此需要考虑相当广泛的使用场景。

应该对大量的事件流有保持高吞吐量，比如日志聚合。

需要优雅的处理大数据的积压，以便支持离线系统数据的定期加载。

同时需要低延时，以便支撑传统的消息队列的应用场景。

我们希望支持分区的/分布式的/实时的流处理去创建新的继承流。这个驱动我们的分区（partitioning）和消费者（consumer）模型。

最终数据流被发送到其他的数据系统服务。系统应该能够容错，当出现机器失败的过程中。

为了支持这些用例导致我们设计一些独一无二的组件，相比传统的消息队列更像是数据库的日志系统。我们将在下面的章节指出设计元素。

### 存储

**不要担心文件系统!**

kafka严重依赖文件系统存储和缓存消息。人们通常有一个"磁盘很慢"的意识，对使用文件系统作为持久化结构持怀疑态度。事实上磁盘要比人们期待的慢得多或快得多，这取决于使用方式；好的磁盘设计的存储结构可以像网络一样快。

过去十年关于磁盘性能的事实是硬盘驱动的吞吐量和磁盘寻址延迟之间的巨大差异。在配置7200转，SATA RAID-5 array的一堆磁盘[JBOD](https://en.wikipedia.org/wiki/Non-RAID_drive_architectures)顺序写入的速率为600M/s而随机写的速率为100k/s，相差6000倍。在硬盘所有的使用模式中这些线性的读写是最容易预测的，同时被操作系统很好的优化了。现代的操作系统提供预读(read-ahead)和后写(write-behind)技术，以要读去的大小的倍数来读取数据，积累小块的写组成一个大块的写写入磁盘。这个问题的进一步讨论可以在[ACM Queue article](http://queue.acm.org/detail.cfm?id=1563874)中找到，他们实际上发现了这篇文章，某种情况下顺序的硬盘访问设置比随机的内存访问更快[sequential disk access can in some cases be faster than random memory access!](http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg)

为了弥补性能差异，现代操作系统在使用主存作为硬盘缓存的时候表现的很激进。一个现代的操作系统会很高兴将所有的可用内存转移为硬盘的缓存，当然当内存被声明使用时这会伴随少量的额外性能损失。所有的硬盘的读写都会通过这个统一的缓存。除非使用直接IO(dirct I/O)，否则这个特性很难关闭，即使进程内部缓存了一份数据，这份数据仍然会被操作系统重复缓存在页缓存中。有效的将所有的东西都缓存了两次。

此外，我们建立在JVM之上，接触过java内存使用的人都知道的两点：

1) 对象的内存开销非常高，通常是对象大小的一倍(或者更糟)。

2) 随着堆内存的增加，java的gc变得越来越繁琐和缓慢。

基于以上事实，使用文件系统以及依靠页缓存要优于维持一个内存缓存或其他结构-直接使用内存的方式会直接导致两倍的可用存储，如果要存储一个压缩的对象可能又会导致缓存加倍。这样做在没有gc惩罚的情况下32G的机器会缓存到28-30G。另外即使服务重启，缓存还是热的，然而进程内缓存需要在内存中重建(10G缓存可能需要10分钟)或者完全冷启动(意味着糟糕的初始化性能)。这也极大的简化了代码，因为用于维护缓存和文件系统之间一致性的所有逻辑现在都在操作系统中，这往往比一次性的进程内尝试更有效、更正确。如果你的磁盘使用偏好线性读取，那么预读实际上就是在每次磁盘读取时使用有用的数据预先填充此缓存。

这暗示了一种非常简单的设计: 相比起来尽可能多的在内存中缓存并刷新到文件系统导致可能的用尽内存的恐慌，我们反着来。所有数据立刻写到文件系统上的持久化日志，不必刷新到磁盘。实际上这只意味着传送到内核的页缓存。

这种以页面缓存为中心的设计在[这篇文章](http://varnish-cache.org/wiki/ArchitectNotes)中有描述。

**恒定时间**

使用在消息系统中的持久化数据结构通常是每个消费者队列伴随一个BTree或其他一般意义上的随机存储结构来保持消息的元数据。BTree是普适性最好的可用数据结构，在消息系统中可以支持事务的或非事务的多种情况。他们确实有相当高的成本，尽管BTree的操作是O(logN)。一般情况下O(logN)基本等同于常数时间，但是对于磁盘操作来说并不是这样。磁盘寻道10ms一次弹出，每个磁盘在一个时间只能进行一次寻道，所以并发是受限的。因此即便一次小的磁盘寻道也会导致很高的开销。存储系统混合了很快的缓存操作和很慢的物理磁盘操作，随着固定缓存的数据增长，可以观测到的树形结构的性能通常是超线性的。

直观的可以类似日志系统的方式建立在简单的读和追加文件上。这个结构的优势在于所有的操作都是O(1)的，且读写不会相互影响。这将有明显的效率改进，由于影响效率的操作完全和数据大小解藕了。服务可以完全发挥便宜/低转速1+TB SATA的驱动。即便他们有低效的寻道效率，这些驱动有大块读写的可接受的性能，且价格只是1/3容量是3倍。

在没有效率损失的情况下接入无限制的虚拟统一磁盘，意味着我们可以提供一些常规消息系统通常找不到的特性。例如，在kafka中当消息被消费后我们可以不删除消息，可以保留相对来说长一点的时间(比如一个星期)。像我们描述的那样，这可以使消费者有极大的灵活性。

### 效率

我们在效率上付出很大的努力。一个主要使用目标是网站活动数据，这个量很大:每次页面访问导致数十次写入。另外我们假设每个发布的消息被至少一个消费者消费(经常是更多)。因此我们努力让消费的代价尽量低廉。

我们同时发现，从相似系统的构建和运行看，效率是影响多租户操作的关键。如果下游的基础服务由于上层应用的使用波动很容易变成系统瓶颈，这样小的改动将经常导致问题。通过变得很快我们能确保在应用程序在基础设施之前变得满载，也就是说基础设施不会影响应用程序。这对于运行在中心分区上运行的中心服务支撑数十上百个应用的服务很重要，因为使用场景几乎每天都变。

前面章节我们讨论的磁盘效率。一旦消除了低效的磁盘访问方式，有两个常见的低效的访问原因：过多的小I/O操作和过多的字节拷贝。

为了避免这种情况，我们的协议建立在"message set"之上，自然将消息进行聚合。这样允许网络请求聚合消息减少网络访问的往返次数，而不是每次只发送一个消息。服务一次性追加一个消息块到日志，消费者一次性获得大的线性块。

这个简单的优化产生了数量级的加速。批量操作导致更大的网络包，更大的顺序磁盘操作，连续的内存块等等。所有的这些措施允许kafka将突发的随机写入流转换为流向消费者的线性写入。

另外一个低效操作是字节拷贝。消息速率低的情况下这并不是一个问题，但是高负载情况下影响比较重大。为了避免这种情况，我们采用了一个标准的二进制文件格式，这个格式被生产者broker和消费者共同使用（这样数据块在流动过程中不会被修改）。

broker本身维护的消息日志就是一个文件目录，每个都由一些列消息结合填充，这些消息集合以生产者和消费者相同的文件格式写入磁盘。保持这种通用格式可以优化最重要的操作：持久日志块的网络传输。现代 Unix 操作系统提供了高度优化的代码，用于将数据从页面缓存传输到套接字。在Linux系统中，这个操作是[sendfile system call](http://man7.org/linux/man-pages/man2/sendfile.2.html)

为了理解sendfile的影响，理解从文件到socket的通用数据路径很重要。

1.操作系统在内核空间中将数据从磁盘读入页缓存。

2.应用程序把数据从内核空间读入用户空间缓冲。

3.应用程序将数据写回到内核空间的socket缓冲。

4.操作系统从socket缓冲中拷贝数据到网卡(NIC)缓冲，发送给网络。

这很明显不高效，有四次拷贝和两次系统调用。使用sendfile技术重复的拷贝被避免了，允许系统从页缓冲直接拷贝数据到网卡。因此在如此优化的路径上，仅仅保留最后拷贝到网卡的步骤。

我们期望的一个通用用例是一个topic下多个consumer。使用zero-copy优化技术，数据被拷贝到内核空间一次，每个消费进行复用，而不是每次consumer进行读取时从存储在内存并拷贝的用户空间。这允许了以接近网络链接限制的速率来消费消息。

页缓存和sendfile的这种组合意味着在consumer所在的kafka集群上几乎看不到磁盘的任何读取活动，因为其完全从缓存中读取数据。

更多的sendfile和zero-copy在java中的支持，可以参考这篇[文章](https://developer.ibm.com/articles/j-zerocopy/)

**端到端的批量压缩**

在一些情况下，短板不在CPU或磁盘，但是在网络带宽。这些现象在数据管道应用中需要数据中心在广域网中传输数据尤其明显。当然，用户始终可以一次压缩一条消息，而无需kafka的任何支持，但这会导致压缩率特别低，因为大部分的冗余是由于不同消息间的重复造成的（例如JSON 中的字段名称或 Web 日志中的用户代理或常见字符串值）。有效的压缩依赖于一起压缩多条消息而不是单独压缩每条消息。

kafka使用高效率的批压缩格式支持。一批消息可以聚集在一起压缩并以这种形式发送到服务器。这批消息要以压缩方式写到日志中，同时将会被consumer解压。

kafka支持GZIP，Snappy，LZ4和ZStardard压缩协议。更多压缩细节参见[这里](https://cwiki.apache.org/confluence/display/KAFKA/Compression)

### 生产者（Producer）

**负载均衡**

生产者直接发送数据给作为分区领导的broker，而不需要中间路由。为了帮助生产者实现这样的功能，所有的kafka节点需要在任何时间都能回答哪个服务是存活的，以及一个topic下的分区领导者是谁，以允许生产者直链这个请求。

生产者控制发送消息到哪个分区。可以是随机的，实现了一系列随机的负载均衡算法，或者可以被一些语义分区函数。我们允许用户通过设置键值，以不同的哈希值来指定分区（必要情况下我们可以重载分区函数）。例如如果key被选为UserId，那么所有该用户的消息都将发送到一个分区。这反过来又允许消费者对他们的消费做出局部性假设。这种分区的风格被明确设计为允许消费者进行局部敏感性处理。

**异步发送**

批处理是提高效率的一大因素，为了能够批量，kafka生产者将尝试在内存中积累数据，并在一次请求中发送他们。批处理过程可以被配置为不超过一定量的消息或不超过固定的时长（比如64k或10ms）。这将允许积累更多的字节去发送，以及服务器上更少的大的I/O操作。这个缓冲以牺牲了少许的延迟而增加了吞吐量。

详细的信息在[configuration](https://kafka.apache.org/documentation/#producerconfigs)和[api](http://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html)章节。

### 消费者（Consumer）

kafka的consumer通过向他想消费的分区领导者的broker发送“fetch”请求来消费。消费者在每个请求中指定日志中的offset来取回一大块从那个位置来的日志。因此消费者对这个位置有显著的控制权，如果有需要可以返回来重复消费。

**推和拉**

我们最初考虑的问题是，consumer是否应该从broker拉消息，还是broker给consumer推消息。在这方面kafka遵循了大多数消息队列的传统设计，数据被producer推到broker，消费者从broker拉去数据。一些日志为中心的系统比如[Sucribe](http://github.com/facebook/scribe)和[Apache Flume](http://flume.apache.org/)遵循了一个不同的基于推的路径，数据是被推到下游的。每中方法都有优缺点。基于推的系统很难和各种各样的consumer达成一致，因为是由broker控制数据流速的。目标是让消费者能有最大的可能的消费速率；不幸的是，在推的模式下，当consumer的速率低于生产者时，consumer将变的不知所措（本质上是拒绝服务攻击）。基于拉的系统有一个好的属性，在这种情况下消费者只是落后然后等适时再赶上来。可以通过某种backoff协议来缓解，consumer表明已经不堪重负，但是充分使用转换率（而不是过载）consumer的处理比看起来更棘手。以前构建系统的尝试是我们更加倾向于采用传统的拉模式。

基于拉的系统的另外一个好处是，consumer可以自己聚合发送到consumer的批量数据。基于推的系统必须选择要么一条一条数据给到consumer或者一下给一批数据给consumer，而不知道下游的consumer是否可以一起处理他。如果调整为低延迟，只是传输上一次一条消息，但是如果仍被缓存，那这将是浪费。一个基于拉的设计修复了这种情况，因为consumer总是拉取日志中当前位置的所有可用消息（或者是配置的最大大小）。因此在不引入不必要延迟的情况下获得了更好的批处理。

基于拉系统的缺陷是，如果broker中没有数据，consumer会陷入紧密的无限循环，紧紧等待数据的来临。为了避免这种情况我们在pull请求中有一个参数可以设置consumer阻塞一个“长轮训”来等待数据到达。（并可以设置给定大小的数据来临时结束等待，以保证大的传输块）。

你可能会想到另外一个可能的设计仅有拉模式，点到点的。producer写到本地日志，broker从producer的本地日志拉取，consumer从broker拉取。一个相同“存储转发”（store-and-forward）生产者经常被提议。这很耐人寻味但是并不符合我们的目标场景，我们的目标场景可能有上千个生产者。从我们的经验来看，系统中规模的运行上千个磁盘跨多个应用进行写入存储不会很可靠，操作将是一场噩梦。在实践中我们发现我们可以运行大规模的有着强大的服务质量的管道，而不需要producer持久化。

**Consumer Position**

低于一个消息系统来说跟踪已经消费的内容很重要。

大部分消息系统将哪些消息已经消费了的消息元数据放在broker上。也就是说一旦消息被发送到消费者，broker要么立即本地为消息记录状态，或者等待消费者给一个确认后再记录状态。这是一个相当直观的选择，事实上对于单机服务器还不清楚除了broker这个状态还能记录在哪。由于在许多消息系统中针对数据存储的数据结构扩展性很差，这也是务实的选择-因为broker知道哪些被消费了，他能立刻删除被消费的数据，保持数据大小较小。

也许不明显的是，让broker和consumer达成一致并非小问题。如果broker当消息被网络发送出去后就记录该消息为**consumed**，如果consumer处理失败（应用崩溃，或超时等等），消息将会丢失。为了解决这个问题很多消息系统添加了回执机制，当消息被网络发送后只是表明为**sent**而不是**consumed**；broker等待指定的回执后才将状态置为**consumed**。这个策略解决了上述的问题，但是带来了新的问题。首先消费端在消费完发送回执的过程中可能出错，将会导致消息被消费两次。第二个问题是性能，现在broker必须记录每个消息的不同状态。（首先锁定他，以防止发送两次，然后将他标记为永久消费，以便删除）。必须处理棘手的问题，比如已经发送但是一直没有收到确认的消息。

kafka用不同的方式处理。我们的topic被分到一组完全有序的分区，在给定时间内一个分区只被一个订阅的消费组里的消费者消费。这意味着consumer的消费位置在每个分区中只是一个整数，笑一个要被消费的消息。这使要被消费的状态变得很小，每个分区仅仅是一个整数。可以定时检查该状态。这等效于消息确认，同时代价又很低。

还有另外一个好处，consumer可以估计重新消费已经消费的消息。这违反了队列的公共契约，但是对于许多consumer来说是必不可少的特性。例如消费端在消费了一些数据后发现了消费端的bug，当bug修复后，消费端可以重新消费这些消息。

**离线数据加载**

可扩展的持久化允许consumer可以定期处理批数据，定期将批量数据加载到离线系统，例如Hadoop或者关系型数据库。

在Hadoop的例子中我们通过将负载分拆到各个map任务上，实现并行化处理数据。每个节点/主题/分区组合一个，允许完全并行加载。Hadoop提供任务管理，任务失败后可以重启，而不用担心重复数据-他们仅仅从原位重启。
